{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Install required dependencies to run this notebook **bold text**"
      ],
      "metadata": {
        "id": "rGu6WcwkgAf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Make following parameter 1 if you have restarted using this colab notebook\n",
        "#after a restart of kernel or re-logging into gmail account\n",
        "running_after_kernel_restart=0\n",
        "if(running_after_kernel_restart==1):\n",
        "  ! pip install gradio\n",
        "  ! pip install langchain\n",
        "  ! pip install dotenv\n",
        "  ! pip install\n",
        "  ! pip install sentence_transformers\n",
        "  ! pip install chromadb\n",
        "  ! pip install unstructured\n",
        "  ! pip install youtube-transcript-api\n",
        "  ! pip install faiss\n",
        "  ! pip install faiss-cpu\n"
      ],
      "metadata": {
        "id": "wJffGrUmaWWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In case you are running this notebook, setup source_documents folder in Google Drive, skip this step if you are running it in your local, but make sure to have setup source_documents folder in workspace**"
      ],
      "metadata": {
        "id": "9ZTiXfwmgOw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "%cd /content/drive/MyDrive\n",
        "%pwd\n",
        "#%mkdir source_documents"
      ],
      "metadata": {
        "id": "hTGiXz-UeWrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Langchain libraries imported**"
      ],
      "metadata": {
        "id": "ANzTno-blqda"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTvvVz5PaJka"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import random\n",
        "import time\n",
        "from dotenv import load_dotenv\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.llms import GPT4All, LlamaCpp\n",
        "from langchain.document_loaders import TextLoader, PDFMinerLoader, CSVLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.docstore.document import Document\n",
        "from langchain import HuggingFacePipeline\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "from dotenv import load_dotenv\n",
        "from multiprocessing import Pool\n",
        "from tqdm import tqdm\n",
        "from langchain.document_loaders import (\n",
        "    CSVLoader,\n",
        "    EverNoteLoader,\n",
        "    PDFMinerLoader,\n",
        "    TextLoader,\n",
        "    UnstructuredEmailLoader,\n",
        "    UnstructuredEPubLoader,\n",
        "    UnstructuredHTMLLoader,\n",
        "    UnstructuredMarkdownLoader,\n",
        "    UnstructuredODTLoader,\n",
        "    UnstructuredPowerPointLoader,\n",
        "    UnstructuredWordDocumentLoader,\n",
        ")\n",
        "import os\n",
        "import glob\n",
        "import argparse\n",
        "\n",
        "from time import sleep\n",
        "from multiprocessing import Process\n",
        "chunk_size = 512 #512\n",
        "chunk_overlap = 50\n",
        "from dotenv import load_dotenv\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.llms import GPT4All, LlamaCpp\n",
        "\n",
        "messages = []\n",
        "kill = 0\n",
        "\n",
        "#load_dotenv()\n",
        "\n",
        "model_type = os.environ.get('MODEL_TYPE')\n",
        "model_path = os.environ.get('MODEL_PATH')\n",
        "model_n_ctx = os.environ.get('MODEL_N_CTX')\n",
        "\n",
        "\n",
        "def parse_arguments():\n",
        "    parser = argparse.ArgumentParser(description='privateGPT: Ask questions to your documents without an internet connection, '\n",
        "                                                 'using the power of LLMs.')\n",
        "    parser.add_argument(\"--hide-source\", \"-S\", action='store_true',\n",
        "                        help='Use this flag to disable printing of source documents used for answers.')\n",
        "\n",
        "    parser.add_argument(\"--mute-stream\", \"-M\",\n",
        "                        action='store_true',\n",
        "                        help='Use this flag to disable the streaming StdOut callback for LLMs.')\n",
        "\n",
        "    return parser.parse_args()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Embeddings model object to vectorize documents **"
      ],
      "metadata": {
        "id": "SyqGFcosmc3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "global qa\n",
        "#args = parse_arguments()\n",
        "#embeddings = HuggingFaceEmbeddings(model_name=embeddings_model_name)\n",
        "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "#model_name = \"sentence-transformers/LaBSE\"\n",
        "#model_name= 'intfloat/e5-large-v2'\n",
        "model_name = 'all-MiniLM-L6-v2'\n",
        "model_kwargs = {'device': 'cpu'}\n",
        "encode_kwargs = {'normalize_embeddings': False}\n",
        "hf = HuggingFaceEmbeddings(\n",
        "            model_name=model_name,\n",
        "            model_kwargs=model_kwargs,\n",
        "            encode_kwargs=encode_kwargs\n",
        "        )\n",
        "hf = HuggingFaceEmbeddings()\n",
        "documents = []\n"
      ],
      "metadata": {
        "id": "8StIt8yJmaXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **All the documents are fetched and stored in vector database here ::**"
      ],
      "metadata": {
        "id": "FcNXv4deiuLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a=glob.glob(\"source_documents/*.txt\")\n",
        "for i in range(len(a)):\n",
        "        print(a[i])\n",
        "        documents.extend(TextLoader(a[i]).load())\n",
        "        print(TextLoader(a[i]).load())\n",
        "\n",
        "#a=glob.glob(\"source_documents/*.html\")\n",
        "#for i in range(len(a)):\n",
        "\n",
        " #       documents.extend(UnstructuredHTMLLoader(a[i]).load())\n",
        "\n",
        "a=glob.glob(\"source_documents/*.pdf\")\n",
        "print(a)\n",
        "for i in range(len(a)):\n",
        "\n",
        "        documents.extend(PDFMinerLoader(a[i]).load())\n",
        "\n",
        "a=glob.glob(\"source_documents/*.csv\")\n",
        "for i in range(len(a)):\n",
        "\n",
        "        documents.extend(CSVLoader(a[i]).load())\n",
        "\n",
        "a=glob.glob(\"source_documents/*.ppt\")\n",
        "for i in range(len(a)):\n",
        "\n",
        "        documents.extend(UnstructuredPowerPointLoader(a[i]).load())\n",
        "\n",
        "a=glob.glob(\"source_documents/*.pptx\")\n",
        "for i in range(len(a)):\n",
        "\n",
        "        documents.extend(UnstructuredPowerPointLoader(a[i]).load())\n",
        "\n",
        "\n",
        "a=glob.glob(\"source_documents/*.docx\")\n",
        "for i in range(len(a)):\n",
        "\n",
        "        documents.extend(UnstructuredWordDocumentLoader(a[i]).load())\n",
        "\n",
        "a=glob.glob(\"source_documents/*.ppt\")\n",
        "for i in range(len(a)):\n",
        "\n",
        "        documents.extend(UnstructuredPowerPointLoader(a[i]).load())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "text_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "print(len(texts))\n",
        "db = Chroma.from_documents(texts, hf)\n"
      ],
      "metadata": {
        "id": "xSGmpiX7aK-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LLM object creation starts here ::**"
      ],
      "metadata": {
        "id": "AjdUWAeqifz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_id=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "model_id='digitous/Alpacino30b'\n",
        "#model_id=\"google/flan-t5-base\"\n",
        "#model_id='tiiuae/falcon-40b'\n",
        "model_id=\"google/flan-t5-large\"\n",
        "llm =  HuggingFacePipeline.from_model_id(model_id=model_id, task=\"text2text-generation\", model_kwargs={\"temperature\":3e-1, \"max_length\" : chunk_size}) #, trust_remote_code=True)\n",
        "retriever = db.as_retriever(search_type='similarity', search_kwargs={\"k\": 3} )\n",
        "#do not increase k beyond 3, else\n",
        "callbacks = []\n",
        "#qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"map_reduce\", retriever=retriever, return_source_documents=True)\n",
        "#qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"refine\", retriever=retriever, return_source_documents=True)\n",
        "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "dC8E0g7tidXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Chatbot code with web UI starts here:**"
      ],
      "metadata": {
        "id": "Knxkyfz7iMS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def parse_arguments():\n",
        "    parser = argparse.ArgumentParser(description='privateGPT: Ask questions to your documents without an internet connection, '\n",
        "                                                 'using the power of LLMs.')\n",
        "    parser.add_argument(\"--hide-source\", \"-S\", action='store_true',\n",
        "                        help='Use this flag to disable printing of source documents used for answers.')\n",
        "\n",
        "    parser.add_argument(\"--mute-stream\", \"-M\",\n",
        "                        action='store_true',\n",
        "                        help='Use this flag to disable the streaming StdOut callback for LLMs.')\n",
        "\n",
        "    return parser.parse_args()\n",
        "\n",
        "def chat_gpt(qa, question):\n",
        "    #args = parse_arguments()\n",
        "    query = question\n",
        "    res = qa(query)\n",
        "    answer, docs = res['result'], res['source_documents']\n",
        "\n",
        "\n",
        "    # Print the relevant sources used#ggml-gpt4all-j-v1.3-groovy.bin for the answer\n",
        "    for document in docs:\n",
        "            print(\" Sources \\n> \" + document.metadata[\"source\"] + \":\")\n",
        "            #print(document.page_content)\n",
        "            metadata=document.metadata[\"source\"]\n",
        "            doc=document.page_content\n",
        "\n",
        "    return answer, doc,metadata\n",
        "\n",
        "\n",
        "\n",
        "with gr.Blocks() as mychatbot:  # Blocks is a low-level API that allows\n",
        "                                # you to create custom web applications\n",
        "    chatbot = gr.Chatbot(label=\"Chat with Professor Bhagwan: Ask questions to your documents without an internet connection\")      # displays a chatbot\n",
        "    question = gr.Textbox(label=\"Question\")     # for user to ask a question\n",
        "    clear = gr.Button(\"Clear Conversation History\")  # Clear button\n",
        "    kill = gr.Button(\"Stop Current Search\")  # Clear button\n",
        "\n",
        "    # function to clear the conversation\n",
        "    def clear_messages():\n",
        "        global messages\n",
        "        messages = []    # reset the messages list\n",
        "\n",
        "\n",
        "    def kill_search():\n",
        "        print(\"killing\")\n",
        "        kill =1\n",
        "        exit()    # reset the messages list\n",
        "\n",
        "    def chat(message, chat_history, kill):\n",
        "        global messages\n",
        "        print(\"chat function launched...\")\n",
        "        print(message)\n",
        "        messages.append({\"role\": \"user\", \"content\": message})\n",
        "        response, doc, metadata = chat_gpt(qa, message)\n",
        "\n",
        "\n",
        "        print(\"private gpt response recieved...\")\n",
        "        print(response)\n",
        "        print(\"content....\")\n",
        "        content = response + \"\\n\" + \"Sources:\"+ \"\\n >\" + metadata+ \":\" +\"\\n\"  +\"Content: \"+\"\\n\"  +doc\n",
        "        print(content)\n",
        "        #['choices'][0]['message']['content']\n",
        "\n",
        "\n",
        "        chat_history.append((message, content))\n",
        "        return \"\", chat_history\n",
        "\n",
        "    # wire up the event handler for Submit button (when user press Enter)\n",
        "    question.submit(fn = chat,\n",
        "                    inputs = [question, chatbot],\n",
        "                    outputs = [question, chatbot])\n",
        "\n",
        "    # wire up the event handler for the Clear Conversation button\n",
        "\n",
        "    clear.click(fn = clear_messages,\n",
        "                inputs = None,\n",
        "                outputs = chatbot,\n",
        "                queue = False)\n",
        "    kill.click(fn = kill_search,\n",
        "                inputs = None,\n",
        "                outputs = chatbot,\n",
        "                queue = False)\n",
        "\n",
        "mychatbot.launch(share=True, debug=True)"
      ],
      "metadata": {
        "id": "qd6oZjSL9kf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NoBFUeV5g2UT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_oXvvq07hqHN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}